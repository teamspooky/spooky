{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pre-work</h1>\n",
    "\n",
    "On my laptop I can't run the code for the entire dataframe, so it's going to be on the half and we'll have to run it at th library over more powerfull laptop. Unless one of us has a powerful enough laptop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Processing functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalization Function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization over number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization_word(var):\n",
    "    words = nltk.Text(word_tokenize(((var))))\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization by number of Sentences</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization_sentence(var):\n",
    "    sentences = nltk.Text(sent_tokenize(var))\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Normalization by number of Characters</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalization_character(var):\n",
    "    return len(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cleaning Function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning(var): #takes a string, returns a string \n",
    "    plain_string = \"\"\n",
    "    for x in var:\n",
    "        x = x.lower()\n",
    "        if (( 'a' <= x and x <= 'z') or x == ' '):\n",
    "            plain_string += x\n",
    "        elif x == '\\'': #we replace the ' by a space \n",
    "            plain_string += ' '\n",
    "    return plain_string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creation of the dataframes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "author_list = ['EAP', 'HPL', 'MWS']\n",
    "train.text= train.text.astype(str)\n",
    "train.author = pd.Categorical(train.author)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "train = train[0:100]  #size reduction for coding\n",
    "\n",
    "train_back_up = train.copy()     #back up is used to define the future dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this cell is just for coding and testing the functions\n",
    "\n",
    "test = train['text'][0]\n",
    "test2 = train['text'][39] #with \"blabla\"\n",
    "test3 = train['text'][70] #with 2 times \"blabla\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Splitting the training set.</h1>\n",
    "<br>\n",
    "Because we have only two dataset, one for training and the other for the Kaggle test. We need to split our 'Kaggle training' set (called t0) into training (called tr1) (in the sense of the training of our predictive model) and testing set (called ts1) ( in the sense of testing our models, and not be tested by Kaggle ! ). <br>\n",
    "<br>\n",
    "We shall notice we will choose the best classifier only with the TR1 DataSet. Then, we will test this classifier with TS1 to see if we over-fitted over TR1.<br>\n",
    "<br>\n",
    "\n",
    "Once we chose the classifier, checked the over-fitting, we will be able to train a the chosen classifier over TR1 and TS1. And so, we'll be able to predict over the \"Kaggle Test Dataset\" and submit the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "14684\n",
      "4895\n",
      "0.7499872312171204\n",
      "0.2500127687828796\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_coef = 20/80 #80% for tr1, 20% for ts1   Maybe we'll have to change\n",
    "\n",
    "tr1, ts1 = train_test_split(t0, test_size = split_coef)\n",
    "\n",
    "#we must study further, it's possible to choose parameters... (TBD)\n",
    "\n",
    "print( len(t0))\n",
    "print(len(tr1))\n",
    "print(len(ts1))\n",
    "print(len(tr1)/len(t0))\n",
    "print(len(ts1)/len(t0))\n",
    "print(len(tr1)/len(t0) + len(ts1)/len(t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Meta-Data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Average sentence length (in characters)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_character(var):#takes string, return int\n",
    "    return len(var)/normalization_sentence(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Average sentence length (in words)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_sentence(var):#takes string, return int\n",
    "    return len(var.split())/normalization_sentence(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Average characters per word</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_word(var):#takes string, return int\n",
    "    return len(var.split())/normalization_word(var) #only word and not punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Punctuation density</h3>\n",
    "Maybe some punctuation of forgot. We should check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_coma(var):#take a string, a punc, return a ratio\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == ',':\n",
    "            cpunc +=1\n",
    "    return cpunc/normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_point(var):#take a string, a punc, return a ratio\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == '.':\n",
    "            cpunc +=1\n",
    "    return cpunc/normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_double_point(var):#take a string, a punc, return a ratio\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == ':':\n",
    "            cpunc +=1\n",
    "    return cpunc/normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_semi_comma(var):#take a string, a punc, return a ratio\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == ';':\n",
    "            cpunc +=1\n",
    "    return cpunc/normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_interro(var):#take a string, a punc, return a ratio\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == '?':\n",
    "            cpunc +=1\n",
    "    return cpunc/normalization_character(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_expl(var):#take a string, a punc, return a ratio\n",
    "    cpunc = 0\n",
    "    for x in var:\n",
    "        if x == '!':\n",
    "            cpunc +=1\n",
    "    return cpunc/normalization_character(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Percentage of unique words per sentence</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocabulary_sentence(var):#take string, return int\n",
    "    var = nltk.Text(sent_tokenize(var))\n",
    "    vocabulary_list = []\n",
    "    for c in var:\n",
    "        if normalization_word(c)!= 0:\n",
    "            vacabulary_count_sentence = len({x.lower() for x in word_tokenize(cleaning(c))})\n",
    "            vocabulary_list.append(vacabulary_count_sentence/normalization_word(c))\n",
    "    return np.mean(vocabulary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Stopword percentage</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_stopword(var):#take a string, return an integer\n",
    "    cs = 0\n",
    "    for x in nltk.Text(word_tokenize(var)):\n",
    "        if x in STOPWORDS:\n",
    "            cs +=1\n",
    "    return cs/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Noun Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_noun(var):\n",
    "    l = []\n",
    "    for x in nltk.pos_tag(word_tokenize(var)):\n",
    "        if x[1][0:2]=='NN': #all the noun tags start with NN\n",
    "            l.append(x)\n",
    "    return len(l)/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Verb Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_verb(var):\n",
    "    l = []\n",
    "    for x in nltk.pos_tag(word_tokenize(var)):\n",
    "        if x[1][0:2]=='VB': #all the noun tags start with NN\n",
    "            l.append(x)\n",
    "    return len(l)/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Adjective Density</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def density_adjective(var):\n",
    "    l = []\n",
    "    for x in nltk.pos_tag(word_tokenize(var)):\n",
    "        if x[1][0:2]=='JJ': #all the noun tags start with NN\n",
    "            l.append(x)\n",
    "    return len(l)/normalization_word(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Adjective to noun ratio</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def adjective_to_noun(var):\n",
    "    return (density_adjective(var))/(density_noun(var)+ 0.5) #because of division by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Building of the Dataframe of Metadata</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#could be modified if we add meta_data\n",
    "list_meta = [length_character,\n",
    "             length_sentence,\n",
    "             length_word,  \n",
    "             vocabulary_sentence,\n",
    "             density_stopword,\n",
    "             density_noun,\n",
    "             density_verb,\n",
    "             density_adjective,\n",
    "             adjective_to_noun,\n",
    "             density_coma,\n",
    "             density_point,\n",
    "             density_double_point,\n",
    "             density_semi_comma,\n",
    "             density_interro,\n",
    "             density_expl\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_meta(dataframe):\n",
    "    df_meta = dataframe.copy()\n",
    "    \n",
    "    for f in list_meta:\n",
    "        df_meta[f.__name__] = df_meta.text.apply(f)\n",
    "        \n",
    "    return df_meta\n",
    "\n",
    "\n",
    "#build_meta(tr1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counting of words (a.k.a bag of words)</h2>\n",
    "\n",
    "Some words about tf_idf: https://buhrmann.github.io/tfidf-analysis.html. <br>\n",
    "Here, we don't detail for each N. I mean we'll run the tf_idf for all the n-grams possible and the we'll take the best features. Our aim is to reduce the size of our dataset (it will be explained further why we should reduce the size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generation of the tf_idf counting dataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def counting_a(a, analyzer_type, process = None, lowercase = True, token_pattern = None):\n",
    "    \n",
    "    \n",
    "    if token_pattern == None:\n",
    "        bow_transformer = CountVectorizer(analyzer = analyzer_type,\n",
    "                                      lowercase = lowercase,\n",
    "                                      ngram_range = (a, a),\n",
    "                                      stop_words='english')\n",
    "    elif token_pattern == 'token_pos':\n",
    "        bow_transformer = CountVectorizer(analyzer = analyzer_type,\n",
    "                                          lowercase = lowercase,\n",
    "                                          ngram_range = (a, a),\n",
    "                                          token_pattern =  r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\.|\\,|\\;|\\:|\\$|\\(|\\)|\\--|\\&|\\``|\\'' + PRP$ + WP$\",\n",
    "                                          stop_words='english')\n",
    "\n",
    "    if process == None: \n",
    "        bow_transformer.fit(tr1['text'])\n",
    "        messages_bow = bow_transformer.transform(tr1['text'])\n",
    "        messages_bow_test = bow_transformer.transform(ts1['text'])  \n",
    "    elif process == 'transform_tag':\n",
    "        bow_transformer.fit(tr1['text'].apply(transform_tag))\n",
    "        messages_bow = bow_transformer.transform(tr1['text'].apply(transform_tag))\n",
    "        messages_bow_test = bow_transformer.transform(ts1['text'].apply(transform_tag))\n",
    "        \n",
    "    #this is the DataFrame Concerning the regular counting of words\n",
    "    tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "    messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "    messages_tfidf_test = tfidf_transformer.transform(messages_bow_test)\n",
    "    \n",
    "    names = bow_transformer.get_feature_names()\n",
    "    \n",
    "    return messages_tfidf, names, messages_tfidf_test\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmat_word, name_word, mat_word_test = counting_a(2,'word')\\n\\nprint ('Shape of Sparse Matrix: ', mat_word.shape)\\nprint ('Amount of Non-Zero occurences: ', mat_word.nnz)\\nprint ('sparsity: %.2f%%' % (100.0 * mat_word.nnz /\\n                             (mat_word.shape[0] * mat_word.shape[1])))\\nprint(' ')\\nprint ('Shape of Sparse Matrix Test: ', mat_word_test.shape)\\nprint ('Amount of Non-Zero occurences: ', mat_word_test.nnz)\\nprint ('sparsity: %.2f%%' % (100.0 * mat_word_test.nnz /\\n                             (mat_word_test.shape[0] * mat_word_test.shape[1])))\\n                             \\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mat_word, name_word, mat_word_test = counting_a(2,'word')\n",
    "\n",
    "print ('Shape of Sparse Matrix: ', mat_word.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_word.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_word.nnz /\n",
    "                             (mat_word.shape[0] * mat_word.shape[1])))\n",
    "print(' ')\n",
    "print ('Shape of Sparse Matrix Test: ', mat_word_test.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_word_test.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_word_test.nnz /\n",
    "                             (mat_word_test.shape[0] * mat_word_test.shape[1])))\n",
    "                             \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get a matrix with <u>roughly a thousand </u>of  features. It's really heavy and we get a sparse matrix. Our goal is now to reduce the size of this matrix by getting the TOP-N features issued from the tf_idf.<br>\n",
    "<br>\n",
    "And we can create so both the matrix of training set. On which the TF_IDF is trained. <br>\n",
    "And the matrix test, which is created with no-fit on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reduction of the number of features to N</h3>\n",
    "\n",
    "The next cell will be called all the bag of words part.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dimensionnality reduction for bag of words (example for 2-grams)</h3>\n",
    "We reduce by taking the TOP-N per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alpha_word = top_feats_by_class(mat_word, tr1.author, name_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_tfidf_classfeats_h(alpha_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building of bag of word and feature vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counting_a(1,'word', process = None, lowercase = True, token_pattern = None)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_bag_a(a,  analyzer_type, process = None, lowercase = True, token_pattern = None):\n",
    "    build = counting_a(a, \n",
    "                     'word', \n",
    "                     process = process, \n",
    "                     lowercase = lowercase, \n",
    "                     token_pattern = token_pattern)\n",
    "    \n",
    "    alpha = top_feats_by_class(build[0], tr1.author, build[1])\n",
    "\n",
    "    a = list(alpha[0].feature.values)\n",
    "    b = list(alpha[1].feature.values)\n",
    "    c = list(alpha[2].feature.values)\n",
    "    bag = set(a + b + c)\n",
    "    df_bag = tr1.copy()\n",
    "    df_bag_test = ts1.copy()\n",
    "\n",
    "    for w in bag:\n",
    "        vec = build[0][:, build[1].index(w)].toarray()\n",
    "        df_bag[w] = vec\n",
    "\n",
    "        vec_test = build[2][:, build[1].index(w)].toarray()\n",
    "        df_bag_test[w] = vec_test\n",
    "        \n",
    "        \n",
    "    return df_bag, df_bag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(set(list(build_bag_a(2, 'word')[1]))-set(list(build_bag_a(2, 'word')[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we get the top N-gramms of words. Our problem is the depedency on the topic. We need to produce other features which are less dependant on the topic. There are sereveral possibility. Let's detail: <br>\n",
    "- Stemming<br>\n",
    "- Character Counting<br>\n",
    "- Pos_tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counting of Stem (TBD) (a.k.a bag of character)</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Counting of Character (a.k.a bag of character)</h2>\n",
    "\n",
    "Here, we count the use of some caracter, and n-grams of caracter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Generation of the tf_idf counting dataFrame</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmat_char, name_char, mat_char_test = counting_a(3, 'word')\\n\\nprint ('Shape of Sparse Matrix: ', mat_char.shape)\\nprint ('Amount of Non-Zero occurences: ', mat_char.nnz)\\nprint ('sparsity: %.2f%%' % (100.0 * mat_char.nnz /\\n                             (mat_char.shape[0] * mat_char.shape[1])))\\nprint(' ')\\nprint ('Shape of Sparse Matrix Test: ', mat_char_test.shape)\\nprint ('Amount of Non-Zero occurences: ', mat_char_test.nnz)\\nprint ('sparsity: %.2f%%' % (100.0 * mat_char_test.nnz /\\n                             (mat_char_test.shape[0] * mat_char_test.shape[1])))\\n                             \\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mat_char, name_char, mat_char_test = counting_a(3, 'word')\n",
    "\n",
    "print ('Shape of Sparse Matrix: ', mat_char.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_char.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_char.nnz /\n",
    "                             (mat_char.shape[0] * mat_char.shape[1])))\n",
    "print(' ')\n",
    "print ('Shape of Sparse Matrix Test: ', mat_char_test.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_char_test.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_char_test.nnz /\n",
    "                             (mat_char_test.shape[0] * mat_char_test.shape[1])))\n",
    "                             \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Dimensionnality reduction for bag of characters example for 3 gram</h3>\n",
    "We reduce by taking the TOP-N per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alpha_char = top_feats_by_class( mat_char, tr1.author, name_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_tfidf_classfeats_h(alpha_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blablablablablabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building of bag of character and feature vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build_bag_a(3, 'char')[0].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Counting of POS Tag (a.k.a bag of Tag)</h2>\n",
    "\n",
    "Here, we count the use of some caracter, and n-grams of caracter.<br>\n",
    "So we have, some non-topic sensitive features.<br>\n",
    "But we can produce an other type of feature based on the POS_tagging.\n",
    "\n",
    "POS tag features. \n",
    "\n",
    "We will check the occurence of the elements from the Upenn Tagset. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS|\\TO|\\VBN|\\''|\\WP|\\UH|\\VBG|\\JJ|\\VBZ|\\--|\\VBP|\\NN|\\DT|\\PRP|\\:|\\WP$|\\NNPS|\\PRP$|\\WDT|\\(|\\)|\\.|\\,|\\``|\\$|\\RB|\\RBR|\\RBS|\\VBD|\\IN|\\FW|\\RP|\\JJR|\\JJS|\\PDT|\\MD|\\VB|\\WRB|\\NNP|\\EX|\\NNS|\\SYM|\\CC|\\CD|\\POS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "#list of all the possible tag names\n",
    "print(\"|\\\\\".join(list(tagdict)))\n",
    "\n",
    "#this is the list of the different tokens we will use.\n",
    "len(tagdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Function to transform a string to a string of Pos_tag</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DT NN , RB , VBD PRP DT NNS IN VBG DT NNS IN PRP$ NN : IN PRP MD VB PRP$ NN , CC NN TO DT NN NN PRP VBP RP , IN VBG JJ IN DT NN : RB RB JJ VBD DT NN .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_tag(var):\n",
    "    inpt = nltk.pos_tag(word_tokenize(var))\n",
    "    unzipped = zip(*inpt )\n",
    "    return ' '.join([*list(unzipped)[1]])\n",
    "transform_tag(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Generation of the tf_idf counting dataFrame</h3>\n",
    "We had to adapt the arguments passed in the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmat_pos, name_pos, mat_pos_test = counting_a(2, 'word', \\n                             process = 'transform_tag',\\n                             lowercase = False, \\n                             token_pattern = 'token_pos')\\n\\nprint ('Shape of Sparse Matrix: ', mat_pos.shape)\\nprint ('Amount of Non-Zero occurences: ', mat_pos.nnz)\\nprint ('sparsity: %.2f%%' % (100.0 * mat_pos.nnz /\\n                             (mat_pos.shape[0] * mat_pos.shape[1])))\\n\\nprint ('Shape of Sparse Test Matrix: ', mat_pos_test.shape)\\nprint ('Amount of Non-Zero occurences: ', mat_pos_test.nnz)\\nprint ('sparsity: %.2f%%' % (100.0 * mat_pos_test.nnz /\\n                             (mat_pos_test.shape[0] * mat_pos_test.shape[1])))\\n                             \\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "mat_pos, name_pos, mat_pos_test = counting_a(2, 'word', \n",
    "                             process = 'transform_tag',\n",
    "                             lowercase = False, \n",
    "                             token_pattern = 'token_pos')\n",
    "\n",
    "print ('Shape of Sparse Matrix: ', mat_pos.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_pos.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_pos.nnz /\n",
    "                             (mat_pos.shape[0] * mat_pos.shape[1])))\n",
    "\n",
    "print ('Shape of Sparse Test Matrix: ', mat_pos_test.shape)\n",
    "print ('Amount of Non-Zero occurences: ', mat_pos_test.nnz)\n",
    "print ('sparsity: %.2f%%' % (100.0 * mat_pos_test.nnz /\n",
    "                             (mat_pos_test.shape[0] * mat_pos_test.shape[1])))\n",
    "                             \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dimensionnality reduction for bag of tags Example for 2 grams</h3>\n",
    "We reduce by taking the TOP-N per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alpha_pos = top_feats_by_class(mat_pos, tr1.author, name_pos, top_n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_tfidf_classfeats_h(alpha_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building of bag of tag and feature vectors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build_bag_a(2,'word', \n",
    "#          process = 'transform_tag',\n",
    "#          lowercase = False, \n",
    "#          token_pattern = 'token_pos')[0].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Positivness/negativness</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weighted sentiment analysis using Vader</h4>\n",
    "Vader contains a list of 7500 features weighted by how positive or negative they are</h4>\n",
    "<br>It uses these features to calculate stats on how positive, negative and neutral a passage is<br>\n",
    "<br>And combines these results to give a compound sentiment (higher = more positive) for the passage<br>\n",
    "<br>Human trained on twitter data and generally considered good for informal communication<br>\n",
    "<br>10 humans rated each feature in each tweet in context from -4 to +4</h4>\n",
    "<br>Calculates the sentiment in a sentence using word order analysis</h4>\n",
    "<br>\"marginally good\" will get a lower positive score than \"extremely good\"\n",
    "<br>Computes a \"compound\" score based on heuristics (between -1 and +1)</h4>\n",
    "<br>Includes sentiment of emoticons, punctuation, and other 'social media' lexicon elements<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vader_comparison(texts):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    headers = ['pos','neg','neu','compound']\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentences = sent_tokenize(texts)\n",
    "    pos=compound=neu=neg=0\n",
    "    num_sentences = len(sentences)\n",
    "    for sentence in sentences:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pos+=vs['pos']/num_sentences\n",
    "        compound+=vs['compound']/num_sentences\n",
    "        neu+=vs['neu']/num_sentences\n",
    "        neg+=vs['neg']/num_sentences\n",
    "    return pos, neg, neu, compound\n",
    "\n",
    "def density_positive(var):\n",
    "    return vader_comparison(var)[0]\n",
    "\n",
    "def density_negative(var):\n",
    "    return vader_comparison(var)[1]\n",
    "\n",
    "def density_neutral(var):\n",
    "    return vader_comparison(var)[2]\n",
    "\n",
    "def density_compound(var):\n",
    "    return vader_comparison(var)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_sensi(dataframe):\n",
    "    df_sen = dataframe.copy() #change here to make sense\n",
    "\n",
    "    df_sen[density_positive.__name__] = df_sen.text.apply(density_positive)\n",
    "    df_sen[density_negative.__name__] = df_sen.text.apply(density_negative)\n",
    "    df_sen[density_neutral.__name__] = df_sen.text.apply(density_neutral)\n",
    "    df_sen[density_compound.__name__] = df_sen.text.apply(density_compound)\n",
    "    return df_sen\n",
    "\n",
    "#build_sensi(tr1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have all the differents features to add but I'd like to focus on the predections..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Fusion of the bunch of features.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we build here the 2 feature datasets (one for TR1, one for TS1)\n",
    "#should be adapted when we'll add features\n",
    "\n",
    "def build_bunch_tr1(dataframe):\n",
    "    list_df = [build_sensi(dataframe),\n",
    "               build_meta(dataframe),\n",
    "               build_bag_a(1,'word')[0],\n",
    "               build_bag_a(2,'word')[0],\n",
    "               build_bag_a(3,'word')[0],\n",
    "               build_bag_a(1,'char')[0],\n",
    "               build_bag_a(2,'char')[0],\n",
    "               build_bag_a(3,'char')[0],\n",
    "               build_bag_a(1,\n",
    "                         'word', \n",
    "                         process = 'transform_tag',\n",
    "                         lowercase = False, \n",
    "                         token_pattern = 'token_pos')[0],\n",
    "               build_bag_a(2,\n",
    "                         'word', \n",
    "                         process = 'transform_tag',\n",
    "                         lowercase = False, \n",
    "                         token_pattern = 'token_pos')[0],\n",
    "               build_bag_a(3,\n",
    "                         'word', \n",
    "                         process = 'transform_tag',\n",
    "                         lowercase = False, \n",
    "                         token_pattern = 'token_pos')[0]\n",
    "              ]\n",
    "\n",
    "    bunch = pd.merge(list_df[0], list_df[1])\n",
    "    \n",
    "    for i in range(2, len(list_df)):\n",
    "        bunch = pd.merge(bunch, list_df[i])\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "def build_bunch_ts1(dataframe):\n",
    "    list_df = [build_sensi(dataframe),\n",
    "               build_meta(dataframe),\n",
    "               build_bag_a(1,'word')[1], \n",
    "               build_bag_a(2,'word')[1], \n",
    "               build_bag_a(3,'word')[1], \n",
    "               build_bag_a(1,'char')[1],\n",
    "               build_bag_a(2,'char')[1],\n",
    "               build_bag_a(3,'char')[1], \n",
    "               build_bag_a(1,'word', \n",
    "                     process = 'transform_tag',\n",
    "                     lowercase = False, \n",
    "                     token_pattern = 'token_pos')[1],\n",
    "               build_bag_a(2,'word', \n",
    "                     process = 'transform_tag',\n",
    "                     lowercase = False, \n",
    "                     token_pattern = 'token_pos')[1],\n",
    "               build_bag_a(3,'word', \n",
    "                     process = 'transform_tag',\n",
    "                     lowercase = False, \n",
    "                     token_pattern = 'token_pos')[1]\n",
    "              ]\n",
    "\n",
    "    bunch = pd.merge(list_df[0], list_df[1])\n",
    "    \n",
    "    for i in range(2, len(list_df)):\n",
    "        bunch = pd.merge(bunch, list_df[i])\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "\n",
    "\n",
    "#df_feat_tr1 = build_bunch_tr1(tr1)\n",
    "#df_feat_ts1 = build_bunch_ts1(ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint ('Shape of df_feat_tr1 Matrix: ', df_feat_tr1.shape)\\nprint ('Amount of Non-Zero occurences: ', np.count_nonzero(df_feat_tr1.values))\\nprint ('sparsity: %.2f%%' % (100.0 * np.count_nonzero(df_feat_tr1.values) /\\n                                 (df_feat_tr1.shape[0] * df_feat_tr1.shape[1])))\\nprint('')\\n\\nprint ('Shape of df_feat_ts1 Matrix: ', df_feat_ts1.shape)\\nprint ('Amount of Non-Zero occurences: ', np.count_nonzero(df_feat_ts1.values))\\nprint ('sparsity: %.2f%%' % (100.0 * np.count_nonzero(df_feat_ts1.values) /\\n                                 (df_feat_ts1.shape[0] * df_feat_ts1.shape[1])))\\n                                 \\n\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print ('Shape of df_feat_tr1 Matrix: ', df_feat_tr1.shape)\n",
    "print ('Amount of Non-Zero occurences: ', np.count_nonzero(df_feat_tr1.values))\n",
    "print ('sparsity: %.2f%%' % (100.0 * np.count_nonzero(df_feat_tr1.values) /\n",
    "                                 (df_feat_tr1.shape[0] * df_feat_tr1.shape[1])))\n",
    "print('')\n",
    "\n",
    "print ('Shape of df_feat_ts1 Matrix: ', df_feat_ts1.shape)\n",
    "print ('Amount of Non-Zero occurences: ', np.count_nonzero(df_feat_ts1.values))\n",
    "print ('sparsity: %.2f%%' % (100.0 * np.count_nonzero(df_feat_ts1.values) /\n",
    "                                 (df_feat_ts1.shape[0] * df_feat_ts1.shape[1])))\n",
    "                                 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Quantitative parameters transformation</h2>\n",
    "Quantitative parameters were transferred into a numerical vector with min-max normalization\n",
    "from 0 to 1. <br>\n",
    "<br>\n",
    "It is necessary to have normalized vectors before training our dataset to avoid a disequilibrium among the coefficients.\n",
    "\n",
    "http://blog.josephmisiti.com/help-commands-for-doing-machine-learning-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_normalization(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in list(df)[3:]:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        if max_value != min_value:\n",
    "            result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "#build_normalization(df_feat_ts1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we have the normalized features matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prediction: Selection of best set of feature selection technic and prediction model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load and prepare data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "names = author_list\n",
    "dataframe_train = build_normalization(build_bunch_tr1(tr1))\n",
    "array = dataframe_train.values\n",
    "X_train = array[:,3:]\n",
    "Y_train = array[:,2]\n",
    "\n",
    "\n",
    "names = author_list\n",
    "dataframe_test = build_normalization(build_bunch_ts1(ts1))\n",
    "array = dataframe_test.values\n",
    "X_test = array[:,3:]\n",
    "Y_test = array[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14684, 341)\n",
      "(4895, 341)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Selection</h2><br>\n",
    "See:http://scikit-learn.org/stable/modules/feature_selection.html <br><br>\n",
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.<br>\n",
    "<br>\n",
    "Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.<br>\n",
    "<br>\n",
    "Three benefits of performing feature selection before modeling your data are:<br>\n",
    "<br>\n",
    "- <b>Reduces Overfitting:</b> Less redundant data means less opportunity to make decisions based on noise.<br>\n",
    "    <br>\n",
    "- <b>Improves Accuracy:</b> Less misleading data means modeling accuracy improves.<br>\n",
    "    <br>\n",
    "- <b>Reduces Training Time:</b> Less data means that algorithms train faster.\n",
    "<br><br>I will not follow the paper here but rather I'll follow the following website: <br>\n",
    "\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/ <br>\n",
    "see also: https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we will try a bunch of different methods:<br>\n",
    "- Univariate feature selection\n",
    "- Recursive feature elimination\n",
    "- L1-based feature selection\n",
    "- Tree-based feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Number of selected features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Current number of features</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 341 features in total.\n"
     ]
    }
   ],
   "source": [
    "print('We have ' + str(np.shape(X_train)[1]) + ' features in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reduction of  number of feaures</h4>\n",
    "The common possibilities are 10,20, quarter and half of total number of samples. <br>\n",
    "<br>\n",
    "For now, we only explore with quarter, tbd for the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to test our models with [10, 20, 85, 170] features extracted from the 341 previous features\n"
     ]
    }
   ],
   "source": [
    "list_N = [10, 20, int(np.shape(X_train)[1]/4), int(np.shape(X_train)[1]/2)]\n",
    "\n",
    "print('We want to test our models with ' + str(list_N) + ' features extracted from the ' + str(np.shape(X_train)[1]) + ' previous features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Univariate feature selection</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_univ(N):\n",
    "    return SelectKBest(chi2, k=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Recursive feature elimination</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_rec(N):\n",
    "    model_rec = LogisticRegression()\n",
    "    return RFE(model_rec, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Principal Component Analysis</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_pca(N):\n",
    "    return PCA(n_components=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selectioners = []\n",
    "for N in list_N:\n",
    "    selectioners.append(('S_UNIV_{}'.format(N), sel_univ(N)))\n",
    "    selectioners.append(('S_REC_{}'.format(N), sel_rec(N)))\n",
    "    selectioners.append(('S_PCA_{}'.format(N), sel_pca(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predictive Models</h2><br>\n",
    "So here we will try this bunch of feature models:\n",
    "\n",
    "- LogisticRegression\n",
    "- LinearDiscriminantAnalysis\n",
    "- KNeighborsClassifier\n",
    "- DecisionTreeClassifier\n",
    "- GaussianNB\n",
    "- SVC\n",
    "- GradientBoostingClassifier\n",
    "- AdaBoostClassifier\n",
    "- ExtraTreesClassifier\n",
    "- RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM', SVC()))\n",
    "models.append(('GBC', GradientBoostingClassifier()))\n",
    "models.append(('ABC', AdaBoostClassifier()))\n",
    "models.append(('ETC', ExtraTreesClassifier()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "#models.append(('MultiNB', MultinomialNB(alpha=0.03)))\n",
    "#models.append(('Calibrated MultiNB', CalibratedClassifierCV( MultinomialNB(alpha=0.03), method='isotonic')))\n",
    "models.append(('Calibrated BernoulliNB', CalibratedClassifierCV( BernoulliNB(alpha=0.03), method='isotonic')))\n",
    "models.append(('Calibrated Huber', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=1e-4, max_iter=10000, tol=1e-4), method='sigmoid')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pipeline</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves two purposes here:<br>\n",
    "- Convenience and Encapsulation<br>\n",
    "        You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "- Joint parameter selection<br>\n",
    "        You can grid search over parameters of all estimators in the pipeline at once.\n",
    "- Safety<br>\n",
    "        Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipes = []\n",
    "\n",
    "for sel in selectioners:\n",
    "    for mod in models:\n",
    "        pipes.append([sel, mod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Compare set of selection and predictive models</h2>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here, we do ten times :<br>\n",
    "- First we train over 9/10 of the training set (TR1)\n",
    "- We test over the 1/10 remaining of the training set (TR1)<br>\n",
    "<br>\n",
    "But here we have a problem. Actually, the metric we need to mesure the models is 'neg_log_loss', because it is the one used by Kaggle. But it doesn't work and we don't know why. So, the metric used is accuracy. <br>\n",
    "<br>\n",
    "Then, we will take the 10 best models:<br>\n",
    "- We train over the whole train set(TR1)\n",
    "- We test over the whole test set (TS1)\n",
    "- we mesure the best with the neg_log_loss metric (here it's working)<br>\n",
    "Here the goal is to select the best with the Kaggle metric, but also we want to check if there is or not overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selector: S_UNIV_10 Predictive model: LR: Mean=-0.976016 Stand Dev=(0.012029)\n",
      "Selector: S_UNIV_10 Predictive model: LDA: Mean=-0.980364 Stand Dev=(0.011002)\n",
      "Selector: S_UNIV_10 Predictive model: KNN: Mean=-4.657878 Stand Dev=(0.709070)\n",
      "Selector: S_UNIV_10 Predictive model: CART: Mean=-17.622279 Stand Dev=(0.343480)\n",
      "Selector: S_UNIV_10 Predictive model: NB: Mean=-3.914900 Stand Dev=(1.066207)\n",
      "Selector: S_UNIV_10 Predictive model: GBC: Mean=-0.959341 Stand Dev=(0.015049)\n",
      "Selector: S_UNIV_10 Predictive model: ABC: Mean=-1.086934 Stand Dev=(0.001304)\n",
      "Selector: S_UNIV_10 Predictive model: ETC: Mean=-7.043688 Stand Dev=(0.483578)\n",
      "Selector: S_UNIV_10 Predictive model: RFC: Mean=-5.001816 Stand Dev=(0.427824)\n",
      "Selector: S_UNIV_10 Predictive model: Calibrated BernoulliNB: Mean=-1.046667 Stand Dev=(0.014959)\n",
      "Selector: S_UNIV_10 Predictive model: Calibrated Huber: Mean=-0.976132 Stand Dev=(0.011842)\n",
      "Selector: S_REC_10 Predictive model: LR: Mean=-1.012763 Stand Dev=(0.007991)\n",
      "Selector: S_REC_10 Predictive model: LDA: Mean=-1.016941 Stand Dev=(0.007397)\n",
      "Selector: S_REC_10 Predictive model: KNN: Mean=-3.318105 Stand Dev=(2.180437)\n",
      "Selector: S_REC_10 Predictive model: CART: Mean=-5.840370 Stand Dev=(0.252891)\n",
      "Selector: S_REC_10 Predictive model: NB: Mean=-9.667268 Stand Dev=(0.357806)\n",
      "Selector: S_REC_10 Predictive model: GBC: Mean=-0.995488 Stand Dev=(0.012073)\n",
      "Selector: S_REC_10 Predictive model: ABC: Mean=-1.078558 Stand Dev=(0.002671)\n",
      "Selector: S_REC_10 Predictive model: ETC: Mean=-3.893266 Stand Dev=(0.720305)\n",
      "Selector: S_REC_10 Predictive model: RFC: Mean=-2.979966 Stand Dev=(0.430291)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-a3b66ee4d690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                                \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                                scoring = scorer )\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mresults_plot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Get coefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Selim/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "  \n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names_model = []\n",
    "\n",
    "results_plot = []\n",
    "seed = 7\n",
    "count = 0\n",
    "count_plot = 0\n",
    "\n",
    "\n",
    "for feat, model in pipes:\n",
    "    count +=1\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    pipeline = Pipeline([feat, model])\n",
    "    #scorer = make_scorer((metrics.log_loss), labels = [0, 1 , 2])\n",
    "    scorer = 'neg_log_loss'\n",
    "    cv_results = model_selection.cross_val_score(pipeline,\n",
    "                                                 X_train,\n",
    "                                                 Y_train,\n",
    "                                                 cv=kfold,\n",
    "                                                 scoring = scorer )\n",
    "    results.append(cv_results)\n",
    "    results_plot.append(cv_results)\n",
    "    names_model.append(feat[0] + ' ' + model[0])\n",
    "    msg = \"%s: Mean=%f Stand Dev=(%f)\" % ('Selector: ' + feat[0] + ' Predictive model: ' + model[0], cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    if count%(len(pipes)/(len(list_N)))==0:\n",
    "        # boxplot algorithm comparison\n",
    "        fig = plt.figure()\n",
    "        fig.suptitle('Algorithm Comparison when N = {}'.format(list_N[count_plot]))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.boxplot(results_plot)\n",
    "        ax.set_xticklabels(names_model, rotation='vertical')\n",
    "        plt.show()\n",
    "        results_plot = []\n",
    "        count_plot +=1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selection of selection and predictive models</h2>\n",
    "<br> We take the three best estimators in mean. These will be tested other the 'TS1' test dataset. <br>\n",
    "Really not sure about me for the criterion. Because, the variance is important too. We should think about it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_results = []\n",
    "for i in range(len(results)):\n",
    "    list_results.append((names_model[i], results[i].mean()))\n",
    "list_results.sort(key=lambda tup: tup[1], reverse = True)\n",
    "list_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feat_model = list(zip(*list_results[0:10]))[0]\n",
    "selected_feat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing over the Testing dataset</h1> <br>\n",
    "Now, we have to test the top-10 models over the remaining training set. Our aim is to see how are they are behaviouring over an un-known dataset and select the best. We will also, and it's the most import ensure there is no overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>List of selected</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_pipes = []\n",
    "\n",
    "for j in range(len(selected_feat_model)):\n",
    "    for i in range(len(pipes)):\n",
    "        f = pipes[i][0][0] + ' ' + pipes[i][1][0]\n",
    "        if f == selected_feat_model[j]:\n",
    "            best_pipes.append((f, pipes[i]))\n",
    "#print(best_pipes)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "\n",
    "class_names = author_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def final_plot_confusion(Y_test, Y_pred):\n",
    "\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test over Top Pipeline</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss_list = []\n",
    "for i in range(len(best_pipes)):\n",
    "    name = best_pipes[i][0]\n",
    "    pipeline = Pipeline([best_pipes[i][1][0], best_pipes[i][1][1]])\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    Y_pred = pipeline.predict(X_test)\n",
    "    y_prob_output = pipeline.predict_proba(X_test) #this is what we send to kaggle.\n",
    "    result = log_loss(Y_test, y_prob_output)\n",
    "    log_loss_list.append((name, result, pipeline))\n",
    "\n",
    "    \n",
    "log_loss_list.sort(key=lambda tup: tup[1], reverse = False)\n",
    "#log_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "winner = log_loss_list[0]\n",
    "print('The best combination Number, Processing, Model is ' \n",
    "      + winner[0]\n",
    "     + ' with a log_loss score of '\n",
    "     + str(winner[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Confusion Matrix</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = winner[2]\n",
    "pipeline.fit(X_train, Y_train)\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "y_prob_output = pipeline.predict_proba(X_test)  \n",
    "final_plot_confusion(Y_test, Y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Submission to Kaggle</h2>\n",
    "First, we must adapt the features building functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading Test Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading of test dataset\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "test.text= test.text.astype(str)\n",
    "\n",
    "test = test[0:30] #for coding \n",
    "#construction of the feature vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adapting building functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def counting_a_test(a, analyzer_type, process = None, lowercase = True, token_pattern = None):\n",
    "    \n",
    "    \n",
    "    if token_pattern == None:\n",
    "        bow_transformer = CountVectorizer(analyzer = analyzer_type,\n",
    "                                      lowercase = lowercase,\n",
    "                                      ngram_range = (a, a),\n",
    "                                      stop_words='english')\n",
    "    elif token_pattern == 'token_pos':\n",
    "        bow_transformer = CountVectorizer(analyzer = analyzer_type,\n",
    "                                          lowercase = lowercase,\n",
    "                                          ngram_range = (a, a),\n",
    "                                          token_pattern =  r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'|\\.|\\,|\\;|\\:|\\$|\\(|\\)|\\--|\\&|\\``|\\'' + PRP$ + WP$\",\n",
    "                                          stop_words='english')\n",
    "\n",
    "    if process == None: \n",
    "        bow_transformer.fit(train['text'])\n",
    "        messages_bow = bow_transformer.transform(train['text'])\n",
    "        messages_bow_test = bow_transformer.transform(test['text'])  \n",
    "    elif process == 'transform_tag':\n",
    "        bow_transformer.fit(train['text'].apply(transform_tag))\n",
    "        messages_bow = bow_transformer.transform(train['text'].apply(transform_tag))\n",
    "        messages_bow_test = bow_transformer.transform(test['text'].apply(transform_tag))\n",
    "        \n",
    "    #this is the DataFrame Concerning the regular counting of words\n",
    "    tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "    messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "    messages_tfidf_test = tfidf_transformer.transform(messages_bow_test)\n",
    "    \n",
    "    names = bow_transformer.get_feature_names()\n",
    "    \n",
    "    return messages_tfidf, names, messages_tfidf_test\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#should be adapted when we'll add features\n",
    "\n",
    "def build_bag_a_test(a,  analyzer_type, process = None, lowercase = True, token_pattern = None):\n",
    "    build = counting_a_test(a, \n",
    "                     'word', \n",
    "                     process = process, \n",
    "                     lowercase = lowercase, \n",
    "                     token_pattern = token_pattern)\n",
    "    \n",
    "    alpha = top_feats_by_class(build[0], tr1.author, build[1])\n",
    "\n",
    "    a = list(alpha[0].feature.values)\n",
    "    b = list(alpha[1].feature.values)\n",
    "    c = list(alpha[2].feature.values)\n",
    "    bag = set(a + b + c)\n",
    "    df_bag = train.copy()\n",
    "    df_bag_test = test.copy()\n",
    "\n",
    "    for w in bag:\n",
    "        vec = build[0][:, build[1].index(w)].toarray()\n",
    "        df_bag[w] = vec\n",
    "\n",
    "        vec_test = build[2][:, build[1].index(w)].toarray()\n",
    "        df_bag_test[w] = vec_test\n",
    "        \n",
    "        \n",
    "    return df_bag, df_bag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we build here the 2 feature datasets (one for TR1, one for TS1)\n",
    "def build_bunch_train(dataframe):\n",
    "    list_df = [build_sensi(dataframe),\n",
    "               build_meta(dataframe),\n",
    "               build_bag_a_test(1,'word')[0],\n",
    "               build_bag_a_test(2,'word')[0],\n",
    "               build_bag_a_test(3,'word')[0],\n",
    "               build_bag_a_test(1,'char')[0],\n",
    "               build_bag_a_test(2,'char')[0],\n",
    "               build_bag_a_test(3,'char')[0],\n",
    "               build_bag_a_test(1,\n",
    "                         'word', \n",
    "                         process = 'transform_tag',\n",
    "                         lowercase = False, \n",
    "                         token_pattern = 'token_pos')[0],\n",
    "               build_bag_a_test(2,\n",
    "                         'word', \n",
    "                         process = 'transform_tag',\n",
    "                         lowercase = False, \n",
    "                         token_pattern = 'token_pos')[0],\n",
    "               build_bag_a_test(3,\n",
    "                         'word', \n",
    "                         process = 'transform_tag',\n",
    "                         lowercase = False, \n",
    "                         token_pattern = 'token_pos')[0]\n",
    "              ]\n",
    "\n",
    "    bunch = pd.merge(list_df[0], list_df[1])\n",
    "    \n",
    "    for i in range(2, len(list_df)):\n",
    "        bunch = pd.merge(bunch, list_df[i])\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "def build_bunch_test(dataframe):\n",
    "    list_df = [build_sensi(dataframe),\n",
    "               build_meta(dataframe),\n",
    "               build_bag_a_test(1,'word')[1], \n",
    "               build_bag_a_test(2,'word')[1], \n",
    "               build_bag_a_test(3,'word')[1], \n",
    "               build_bag_a_test(1,'char')[1],\n",
    "               build_bag_a_test(2,'char')[1],\n",
    "               build_bag_a_test(3,'char')[1], \n",
    "               build_bag_a_test(1,'word', \n",
    "                     process = 'transform_tag',\n",
    "                     lowercase = False, \n",
    "                     token_pattern = 'token_pos')[1],\n",
    "               build_bag_a_test(2,'word', \n",
    "                     process = 'transform_tag',\n",
    "                     lowercase = False, \n",
    "                     token_pattern = 'token_pos')[1],\n",
    "               build_bag_a_test(3,'word', \n",
    "                     process = 'transform_tag',\n",
    "                     lowercase = False, \n",
    "                     token_pattern = 'token_pos')[1]\n",
    "              ]\n",
    "\n",
    "    bunch = pd.merge(list_df[0], list_df[1])\n",
    "    \n",
    "    for i in range(2, len(list_df)):\n",
    "        bunch = pd.merge(bunch, list_df[i])\n",
    "    \n",
    "    return bunch\n",
    "\n",
    "#df_feat_train = build_bunch_train(train)\n",
    "df_feat_test = build_bunch_test(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training over all the Training Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "names = author_list\n",
    "dataframe_train = build_normalization(build_bunch_train(train))\n",
    "array = dataframe_train.values\n",
    "X_train = array[:,3:]\n",
    "Y_train = array[:,2]\n",
    "\n",
    "names = author_list\n",
    "dataframe_test = build_normalization(build_bunch_test(test))\n",
    "array = dataframe_test.values\n",
    "X_test = array[:,2:]\n",
    "Y_test = array[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "pipeline = winner[2]\n",
    "pipeline.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating the probabilities</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = pipeline.predict(X_test)\n",
    "y_prob_output = pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_prob_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>CSV File</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_output[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame(columns = ['id',\"EAP\",\"HPL\",\"MWS\"])\n",
    "df_submit['id'] = test['id']\n",
    "df_submit['EAP'] = y_prob_output[:,0]\n",
    "df_submit['HPL'] = y_prob_output[:,1]\n",
    "df_submit['MWS'] = y_prob_output[:,2]\n",
    "df_submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "\n",
    "print('Congratulation, project is done ! It took: ' + str(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
