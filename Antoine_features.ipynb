{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "author_list = ['EAP', 'HPL', 'MWS']\n",
    "df.text= df.text.astype(str)\n",
    "df.author = pd.Categorical(df.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_sent(var):\n",
    "    sentences = nltk.Text(sent_tokenize(var))\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'] = df.text.apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['nb_sentence']=df.sentence.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> First word </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_word_type(sents):\n",
    "    res=list()\n",
    "    for sent in sents:\n",
    "        typ=nltk.pos_tag(sent.split())\n",
    "        i=0\n",
    "        j=0\n",
    "        word=typ[i][0].lower()\n",
    "        while i<len(sent.split()):\n",
    "            if j<len(word) and word[j] >= 'a' and word[j] <= 'z':\n",
    "                res.append(typ[i][1])\n",
    "                break\n",
    "            i=i+1\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes a dataframe as input to create the columns for the type of first word of the sentences and set the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_word(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        first_words=first_word_type(dataframe['sentence'].iloc[i])\n",
    "        for word in first_words:\n",
    "            column='first_word_' + str(word)\n",
    "            try:\n",
    "                dataframe[column].iloc[i]=dataframe[column].iloc[i]+1\n",
    "            except:\n",
    "                dataframe[column]=0\n",
    "                dataframe[column].iloc[i]=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_word(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Two first Words </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twofirst_words_type(sents):\n",
    "    res = list()\n",
    "    for sent in sents:\n",
    "        if len(sent.split()) > 1:\n",
    "            typ=nltk.pos_tag(sent.split())\n",
    "            res.append([typ[0][1],typ[1][1]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twofirst_word(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        twofirst_words = twofirst_words_type(dataframe['sentence'].iloc[i])\n",
    "        for twoword in twofirst_words:\n",
    "            column='first_two_' + str(twoword)\n",
    "            try:\n",
    "                dataframe[column].iloc[i]=dataframe[column].iloc[i]+1\n",
    "            except:\n",
    "                dataframe[column]=0\n",
    "                dataframe[column].iloc[i]=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twofirst_word(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Last word </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_word_type(sents):\n",
    "    res=list()\n",
    "    for sent in sents:\n",
    "        typ=nltk.pos_tag(sent.split())\n",
    "        i=0\n",
    "        j=0\n",
    "        word=typ[-i][0].lower()\n",
    "        while i<len(sent.split()):\n",
    "            if j<len(word) and word[j] >= 'a' and word[j] <= 'z':\n",
    "                res.append(typ[-i][1])\n",
    "                break\n",
    "            i=i+1\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_word(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        last_words=last_word_type(dataframe['sentence'].iloc[i])\n",
    "        for word in last_words:\n",
    "            column='last_word_' + str(word)\n",
    "            try:\n",
    "                dataframe[column].iloc[i]=dataframe[column].iloc[i]+1\n",
    "            except:\n",
    "                dataframe[column]=0\n",
    "                dataframe[column].iloc[i]=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last_word(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Two last words </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twolast_words_type(sents):\n",
    "    res = list()\n",
    "    for sent in sents:\n",
    "        if len(sent.split()) > 1:\n",
    "            typ=nltk.pos_tag(sent.split())\n",
    "            res.append([typ[-2][1],typ[-1][1]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twolast_word(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        twolast_words = twolast_words_type(dataframe['sentence'].iloc[i])\n",
    "        for twoword in twolast_words:\n",
    "            column='last_two_' + str(twoword)\n",
    "            try:\n",
    "                dataframe[column].iloc[i]=dataframe[column].iloc[i]+1\n",
    "            except:\n",
    "                dataframe[column]=0\n",
    "                dataframe[column].iloc[i]=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twolast_word(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Foreign Language </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_sent(sents):\n",
    "    res=list()\n",
    "    for sent in sents:\n",
    "        try:\n",
    "            res.append(detect(sent))\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        n=tab['nb_sentence'].iloc[i]\n",
    "        languages=language_sent(dataframe['sentence'].iloc[i])\n",
    "        for language in languages:\n",
    "            if language != 'en':\n",
    "                column=language\n",
    "                try:\n",
    "                    dataframe[column].iloc[i]=dataframe[column].iloc[i]+1/n\n",
    "                except:\n",
    "                    dataframe[column]=0\n",
    "                    dataframe[column].iloc[i]=1/n\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Emotions </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nrc_data():\n",
    "    nrc = \"NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt\"\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        words=tab['text'].iloc[i].split()\n",
    "        n=len(words)\n",
    "        emotion_dic=get_nrc_data()\n",
    "        for word in words:\n",
    "            if word in emotion_dic:\n",
    "                for emotion in emotion_dic['word']:\n",
    "                    column=emotion\n",
    "                try:\n",
    "                    dataframe[column].iloc[i]=dataframe[column].iloc[i]+1/n\n",
    "                except:\n",
    "                    dataframe[column]=0\n",
    "                    dataframe[column].iloc[i]=1/n\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
